\chapter{Background}
\label{ch:background}

\section{RTOS Evaluation}
\label{sec:rtos_evaluation}

Since Rust is a quite new language, there is no dominating open source RTOS yet, as it is the case for FreeRTOS in the Embedded C/C++ world.
Therefore, a few competing RTOS's exist in the Rust embedded world.
The website "Are we RTOS yet?"~\cite{AWRY} lists the most important RTOS's that currently exist for Rust.
One criterion for the selection of an operating system was that it is fully written in Rust.
There exist a few RTOS's that are written in C but feature a Rust wrapper.
However, with this setup, the memory safety guarantees by the compiler are lost.
Therefore, we picked the six RTOS's with the most stars and forks on GitHub for closer investigation. The chosen OSes were: Bern, Drone, Embassy, Hubris, Tock and RTIC. 

\subsection{Bern}
Bern OS was developed in the process of a master thesis. It features multiple isolated processes, which are scheduled with a preemptive round-robin scheduling algorithm, similar to FreeRTOS. However, since it is a relatively new, one-man project, it was far from sure that it would be continued, therefore we did not choose it~\cite{Bern}.

\subsection{Hubris}
Hubris resembles a more classical OS, since it features a kernel that runs in privileged mode, and tasks that are separately complied and run in an unprivileged mode. However, we preferred a simpler, more bare-metal implementation that does not use different privilege modes~\cite{Hubris}. 

\subsection{Tock}
Tock OS was the most popular OS on GitHub, but it turned out that it does not feature prioritized tasks. Since this is essential for an embedded RTOS we did not choose it~\cite{Tock}.

\subsection{Drone/Embassy}
Drone and Embassy both support tasks with different priorities. Tasks with the same priority are cooperatively run using the async/await concept. It would have been an interesting choice, but finally we decided to choose RTIC that was solely based on interrupts.~\cite{Drone},~\cite{Embassy}.

\subsection{RTIC}
\gls{rtic} is a very light weight operating system that does not feature a kernel or scheduler, but outsources the whole OS-functionality to auto generated interrupt handlers.
It is developed by a team of the university of Lule√• in Sweden.
Therefore, long term support seamed likely.
Since it uses the innovative interrupt based task dispatching model, it interacts closely with the hardware and was therefore the optimal choice to identify potential performance issues of the hardware implementation of the ControlPULP IP.

\section{RTIC}
\gls{rtic}, formerly known as RTFM (Real Time For the Masses), provides all features that are needed in an embedded RTOS, while generating minimal overhead.
It features tasks with static priorities. Higher priority tasks can preempt lower priority tasks. Equal priority tasks, however, run sequentially and therefore cooperatively.
There are no message passing queues as for example in FreeRTOS, but tasks can be passed a payload at spawn time.
Furthermore, tasks can access shared resources. However, the list of resources that can be accessed by a specific task has to be given at compile time~\cite{RTIC}. 

\subsection{Code Generation}
Since \gls{rtic} does not feature a kernel, all functionality lies in auto generated interrupt handlers.
The whole \gls{rtic} program has to be defined inside a \texttt{\#app} macro. At compile time, the macro processor performs a static analysis of the program. The number of tasks, their priorities and the resources that can be accessed by the tasks are determined. With this information, the necessary interrupt handlers are generated. They are described in the following.

\subsubsection{Task Dispatcher}
\label{sec:task_dispatcher}
Task dispatchers are responsible for the execution of all tasks of a specific priority.
They are bound to an interrupt that is configured to the same priority.
Each task dispatcher contains a queue with references to all tasks that are ready to be executed at a specific moment.
If a task is to be spawned, a reference to the task is placed in the ready queue of it's corresponding task dispatcher.
Thereafter, the task dispatcher's interrupt is pended in software. The interrupt handler only consists of a loop through the ready queue.
For each reference to a task in the ready queue, a corresponding task instance is run inside the interrupt handler.
With this system, no scheduling algorithm is needed, since higher priority tasks will always preempt, or in this case interrupt, lower priority tasks.
Same priority tasks run cooperatively.

\subsubsection{Timer Handler}
\label{sec:timer_handler}
With task dispatchers, it is only possible to spawn tasks that run immediately.
To spawn tasks in the future, the timer handler is generated. It is an interrupt handler for a timer interrupt.
The timer handler is unique in the system, and handles tasks of all priorities.
The timer interrupt is set to the maximum priority.
If a task spawn is scheduled in the future, a reference to the task is placed in the timer queue of the timer handler, together with its desired spawn time.
As soon as the timer queue is modified, the timer comparison value is adjusted, such that the timer interrupt fires at the spawn time of the task that will start first.
If the timer interrupts and the handler is run, it moves all tasks that are still in the timer queue and now became ready into their corresponding task dispatcher, and pends the task dispatcher's interrupt in software.
Since the timer interrupt is running at maximum priority, this happens immediately.
Since the actual running of the tasks is still handled by the task dispatcher, the priorities are still respected.

\subsubsection{Shared Resources}
\label{sec:shared_resources}
Since it is known at compile time which tasks will access which resources, the locking of resources can be simplified.
During the evaluation of the \texttt{\#app} macro, for each shared resource, a list is generated with all tasks that could access the resource.
Tasks can only be preempted by other tasks, if a task dispatcher interrupt with higher priority fires.


Therefore, to lock a shared resource, one can just temporarily raise the interrupt level threshold of the system to the highest level of any task that also has access to the shared resource. Like this, it is guaranteed, that no other task can access the shared resource while it is locked.
If a task itself is the one with the highest priority, accessing a specific resource, the locking of this resource is completely optimized out at compile time.

\section{Rust Embedded Ecosystem}

\begin{figure}
  \centerfloat
  \includegraphics[width=\textwidth]{fig/crate_hierarchy.pdf}
  % Note: We do not have the SVG source file for the image above, so we have to put the PDF under version control.
  \caption{The Hierarchy of Embedded Crates}%
  \label{fig:embedded_crates}
  % Note: The `\label{}` can be on the line after the `\caption{}` if the `\caption{}` line ends with a comment.
\end{figure}

Since Rust appeared only very recently, the community already gained a lot of experience in embedded development.
Therefore, attempts were made to standardize and categorize embedded libraries.
There are several types of libraries, or crates, as they are called in Rust, see figure \ref{fig:embedded_crates}.
Most of the embedded libraries in Rust are strongly focused on the ARM Cortex-M architecture. Implementations for RISC-V microcontroller did exist, however, they are usually very minimalistic. See section~\ref{sec:libraries} for an overview of the necessary changes for RISC-V.

In the following, the definitions of different embedded Rust crate types are introduced.

\subsection{Micro-architecture Crate (MAC)}
These crates handle any useful routines common to the processor core of a specific microcontroller, as well as any peripherals that are common to all microcontrollers that use that particular type of processor core. For example, the cortex-m crate gives features functions to enable and disable interrupts, which are the same for all Cortex-M based microcontrollers. It also gives you access to the 'SysTick' peripheral that is included in all Cortex-M based microcontrollers~\cite{RustEmbedded}. There also exists a very basic crate for RISC-V.

\subsection{Peripheral Access Crate (PAC)}
These crates are thin wrappers for the various memory-wrapper registers defined for a particular part-number of microcontroller. For example, stm32f30x for the ST-Micro STM32F30x series. Here, registers can be accessed directly, following each peripheral's operating instructions given in the microcontroller's Technical Reference Manual~\cite{RustEmbedded}.

\subsection{HAL Crate}
These crates offer a more user-friendly API for a particular processor, often by implementing some common traits defined in embedded \gls{hal}. It therefore makes use of the trait system, that could be compared to interfaces in Java. For example, this crate might offer a Serial struct, with a constructor that takes an appropriate set of GPIO pins and a baud rate, and offers some sort of \texttt{write_byte} function for sending data~\cite{RustEmbedded}. 

\subsection{Board Crate}
These crates go one step further than a \gls{hal} crate by preconfiguring various peripherals and GPIO pins to suit a specific developer kit or board, such as stm32f3-discovery for the STM32F3DISCOVERY board~\cite{RustEmbedded}.

\subsection{Runtime Crate}
These crates provide all code that is required to get to main. They usually feature either some low level Rust or even assembly code that initializes all registers, such as the stack pointer, setup interrupt vectors and finally call \texttt{main}. Furthermore, they provide macros to specify interrupt handlers. A RISC-V version of a runtime crate does exist, however only in a minimalistic version, without interrupt handler support.

\section{Interrupt Controller}
\label{sec:interrupt_controller}
Since \gls{rtic} heavily relies on interrupts for tasks, it is also very dependent on a powerful interrupt controller hardware.
Originally, \gls{rtic} was designed to be used with the \gls{nvic}.
For RISC-V the \gls{clic} is the most advanced interrupt controller available.
Fortunately, they are quite similar.
In the following, their major features are explained, and the main differences are stressed out~\cite{NVIC}\cite{CLIC}. 

\subsection{Vectorization}
\label{sec:vectorization}
The \gls{nvic} and the \gls{clic} are both vectorized interrupt controllers.
For the \gls{nvic}, this means, that there exists a table of addresses of interrupt handler functions, the vector table. For each interrupt, there exists an entry in the table. If an interrupt occurs, the program counter is set automatically to the address of the interrupt handler that is associated with the interrupt.
According to the specification, the vector table of the \gls{clic} works in the same way.
In the specific implementation of our group, however, it is not filled with addresses, but with jump instructions to the different interrupt handlers.
It has the same effect, but requires a different setup process of the interrupt table.~\cite{NVIC}\cite{CLIC}

\subsection{Configuration}
\label{sec:interrupt_configuration}
Both the \gls{nvic} and the \gls{clic} allow for setting priority levels for interrupts individually.
Higher priority interrupts preempt lower priority interrupts.
Both implementation feature 256 interrupt levels. However, while for the \gls{clic} the most urgent interrupt has priority 255, the \gls{nvic} inverts priority levels, and considers interrupts with the lowest priority value as the most urgent.
Similarly, the triggers can be configured for each interrupt individually. Both \gls{clic} and \gls{nvic} feature level and edge sensitive triggers. For certain implementation of \gls{nvic} and all \gls{clic} implementations, it is possible to set priority threshold. Interrupts with priority below this threshold can be pended, but will not be executed until the threshold is lowered. For both implementations, this threshold can be dynamically changed at runtime.
Furthermore, for the \gls{clic}, interrupts can be configured to be non vectorized, and therefore they will use one single handler function as the default exception handler. 
~\cite{NVIC}\cite{CLIC}.

\section{Register Count}
\label{sec:register_count}
In ARM Cortex-M3 there are 12 general purpose registers, plus 4 additional service registers, like the stack pointer~\cite{ARMRegisters}. In the default RISC-V integer ABI, there are 32 general purpose registers\cite{RISCVIntegerABI}. ARM only features 4 caller saved registers, whereas in RISC-V, there are 16 caller saved registers. Since the caller saved registers must be pushed onto the stack in case of an interrupt context switch, this makes context switches in RISC-V more expensive. There are several measures to reduce the amount of registers that have to be saved during a context switch. The Embedded ABI lowers the amount of caller saved registers to 4~\cite{RISCVEmbeddedABI}. The E-Extension only provides 16 registers~\cite{RISCVEExtension}. The resulting number of caller saved registers can be found in table \ref{tbl:register_counts}.

\begin{table}
  \centerfloat
  \begin{tabular}{ l c c }
    \toprule
    \textbf{Configuration} & \textbf{Total Registers} & \textbf{Caller Saved Registers} \\
    \midrule
    ARM & 16 & 4 \\
    RISC-V Integer ABI & 32 & 16 \\
    RISC-V Embedded ABI & 32 & 4 \\
    RISC-V E-Extension, Integer ABI & 16 & 10 \\
    RISC-V E-Extension, Embedded ABI & 16 & 4 \\
    \bottomrule
  \end{tabular}
  \caption{Register Count}%
  \label{tbl:register_counts}
\end{table}